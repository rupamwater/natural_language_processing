{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f4c101-5395-4bfc-86a8-a76d4436bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d54b81b2-b2db-414f-8f25-f512339928b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 3000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128 # Batch size for training.\n",
    "EPOCHS = 2000 # Number of epochs to train for.\n",
    "LATENT_DIM = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c817e4d-4f51-44e9-a91a-d32cfb176d19",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b4d20b-c21b-427c-ad77-3767efcd939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "for line in open('../hmm_class/robert_frost.txt'):\n",
    "  line = line.rstrip()\n",
    "  if not line:\n",
    "    continue\n",
    "\n",
    "  input_line = '<sos> ' + line\n",
    "  target_line = line + ' <eos>'\n",
    "\n",
    "  input_texts.append(input_line)\n",
    "  target_texts.append(target_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e3f76-537d-4312-9cb7-bbf4d6fba550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_lines = input_texts + target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6b35a5-4a46-48e1-a498-4b4cc24edcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 20000\n"
     ]
    }
   ],
   "source": [
    "# convert the sentences (strings) into integers\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "tokenizer.fit_on_texts(all_lines)\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280a6dc-83d6-479b-92a9-5adbd162201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max seq length\n",
    "max_sequence_length_from_data = max(len(s) for s in input_sequences)\n",
    "print('Max sequence length:', max_sequence_length_from_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f929a9-c44e-4965-8bb2-fe06cab9596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word -> integer mapping\n",
    "word2idx = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2idx))\n",
    "assert('<sos>' in word2idx)\n",
    "assert('<eos>' in word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd02db-bbd5-4a4d-b59e-6043635c3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences so that we get a N x T matrix\n",
    "max_sequence_length = min(max_sequence_length_from_data, MAX_SEQUENCE_LENGTH)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
    "print('Shape of data tensor:', input_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6eb05-d393-40cc-b10b-f558cbdeec78",
   "metadata": {},
   "source": [
    "### Create Word Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1725ed4-5d46-4da3-9cb4-25aefdabd723",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('../large_files/glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb21e3a-dddc-48a0-baf1-0c487c81a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "  if i < MAX_VOCAB_SIZE:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a68b5f6-7abe-4d02-964d-0327084b9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot the targets (can't use sparse cross-entropy)\n",
    "one_hot_targets = np.zeros((len(input_sequences), max_sequence_length, num_words))\n",
    "for i, target_sequence in enumerate(target_sequences):\n",
    "  for t, word in enumerate(target_sequence):\n",
    "    if word > 0:\n",
    "      one_hot_targets[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95afda-78a0-4f1f-980c-501e39da2540",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac064ca-8ff2-4274-926e-d3611b21bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  # trainable=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4c7ff-9863-4183-b583-7a377933f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building model...')\n",
    "\n",
    "# create an LSTM network with a single LSTM\n",
    "input_ = Input(shape=(max_sequence_length,))\n",
    "initial_h = Input(shape=(LATENT_DIM,))\n",
    "initial_c = Input(shape=(LATENT_DIM,))\n",
    "x = embedding_layer(input_)\n",
    "lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "x, _, _ = lstm(x, initial_state=[initial_h, initial_c]) # don't need the states here\n",
    "dense = Dense(num_words, activation='softmax')\n",
    "output = dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345def13-fc09-496c-a0f6-fcd5556590b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([input_, initial_h, initial_c], output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510607f3-4357-4454-b3cf-4e3114372c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  # optimizer='rmsprop',\n",
    "  optimizer=Adam(lr=0.01),\n",
    "  # optimizer=SGD(lr=0.01, momentum=0.9),\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5fe05-0792-4ffa-a702-9b2deaf231de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training model...')\n",
    "z = np.zeros((len(input_sequences), LATENT_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781efcb-f35e-4cd3-804d-32bd7b5ab51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.fit(\n",
    "  [input_sequences, z, z],\n",
    "  one_hot_targets,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=VALIDATION_SPLIT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cc47c-74ee-4ce2-b62f-4346f20898e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb4276-cdac-48fa-bbca-ed6cde740096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies\n",
    "plt.plot(r.history['accuracy'], label='acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81546f89-ebcf-4a7c-a282-92695187ccdb",
   "metadata": {},
   "source": [
    "## Build the Generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c90097-411a-4ade-9e5d-5d02d58a640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input2 = Input(shape=(1,)) # we'll only input one word at a time\n",
    "x = embedding_layer(input2)\n",
    "x, h, c = lstm(x, initial_state=[initial_h, initial_c]) # now we need states to feed back in\n",
    "output2 = dense(x)\n",
    "sampling_model = Model([input2, initial_h, initial_c], [output2, h, c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15a2cc-119e-44d2-8a48-2b4976116364",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v:k for k, v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd44b5c-73ac-4cf4-9e10-2b978b11ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_line():\n",
    "  # initial inputs\n",
    "  np_input = np.array([[ word2idx['<sos>'] ]])\n",
    "  h = np.zeros((1, LATENT_DIM))\n",
    "  c = np.zeros((1, LATENT_DIM))\n",
    "\n",
    "  # so we know when to quit\n",
    "  eos = word2idx['<eos>']\n",
    "\n",
    "  # store the output here\n",
    "  output_sentence = []\n",
    "\n",
    "  for _ in range(max_sequence_lengthal):\n",
    "    o, h, c = sampling_model.predict([np_input, h, c])\n",
    "\n",
    "    # print(\"o.shape:\", o.shape, o[0,0,:10])\n",
    "    # idx = np.argmax(o[0,0])\n",
    "    probs = o[0,0]\n",
    "    if np.argmax(probs) == 0:\n",
    "      print(\"wtf\")\n",
    "    probs[0] = 0\n",
    "    probs /= probs.sum()\n",
    "    idx = np.random.choice(len(probs), p=probs)\n",
    "    if idx == eos:\n",
    "      break\n",
    "\n",
    "    # accuulate output\n",
    "    output_sentence.append(idx2word.get(idx, '<WTF %s>' % idx))\n",
    "\n",
    "    # make the next input into model\n",
    "    np_input[0,0] = idx\n",
    "\n",
    "  return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ffe78-9c3c-4917-b9e6-4dfc037afabd",
   "metadata": {},
   "source": [
    "### Generate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60a230-accc-44eb-9fbd-8648d44e8894",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "  for _ in range(4):\n",
    "    print(sample_line())\n",
    "\n",
    "  ans = input(\"---generate another? [Y/n]---\")\n",
    "  if ans and ans[0].lower().startswith('n'):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b73b5-b18d-446b-9ff7-2eb800437ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
