{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f4c101-5395-4bfc-86a8-a76d4436bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d54b81b2-b2db-414f-8f25-f512339928b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # Batch size for training.\n",
    "EPOCHS = 40  # Number of epochs to train for.\n",
    "LATENT_DIM = 256  # Latent dimensionality of the encoding space.\n",
    "NUM_SAMPLES = 10000  # Number of samples to train on.\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c817e4d-4f51-44e9-a91a-d32cfb176d19",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b4d20b-c21b-427c-ad77-3767efcd939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where we will store the data\n",
    "input_texts = [] # sentence in original language\n",
    "target_texts = [] # sentence in target language\n",
    "target_texts_inputs = [] # sentence in target language offset by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6b35a5-4a46-48e1-a498-4b4cc24edcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 20000\n"
     ]
    }
   ],
   "source": [
    "t=0\n",
    "for line in open('../fra-eng/fra.txt'):\n",
    "    t += 1\n",
    "    #print(line)\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    input_text, translation, *rest = line.rstrip().split('\\t')\n",
    "    #print(input_text, translation )\n",
    "\n",
    "    # make the target input and output\n",
    "    target_text = translation + ' <eos>'\n",
    "    target_text_input = '<sos> ' + translation\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)\n",
    "    \n",
    "print(\"num samples:\", len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "367cd069-c1db-4fb5-9ba4-ac6a4995c306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439403a-7188-4c0f-895c-42d2a349d56f",
   "metadata": {},
   "source": [
    "### Process input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e09cb04-756d-48a2-892b-43b13c99106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "max_len_input = max(len(s) for s in input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74c034-8f69-400e-97e4-d5b7aa5286ec",
   "metadata": {},
   "source": [
    "### Process output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cf2dada-e970-4f33-9eec-28d7ffd440a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
    "\n",
    "\n",
    "# get the word to index mapping for output language\n",
    "word2idx_outputs = tokenizer_outputs.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d0a2136-dc98-46a0-844a-3909ae373d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_output = len(word2idx_outputs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c37b7b37-411b-466f-9c8a-321d357c1d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "141b3864-2430-49b0-9d20-f6f17e916816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs.shape: (20000, 4)\n",
      "encoder_inputs[0]: [ 0  0  0 10]\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_inputs[0]:\", encoder_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "584371e4-be64-42f5-8804-6cae3fa9a970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_inputs[0]: [ 2 42  4  0  0  0  0  0  0  0  0]\n",
      "decoder_inputs.shape: (20000, 11)\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
    "print(\"decoder_inputs.shape:\", decoder_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd489563-1308-46d5-a3e8-efd98bcad735",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00b450-d475-469a-bacd-c1f8753ff035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets, since we cannot use sparse\n",
    "# categorical cross entropy when we have sequences\n",
    "decoder_targets_one_hot = np.zeros(\n",
    "  (\n",
    "    len(input_texts),\n",
    "    max_len_target,\n",
    "    num_words_output\n",
    "  ),\n",
    "  dtype='float32'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ea461-fbde-43fc-bfe7-15d8e72355a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the values\n",
    "for i, d in enumerate(decoder_targets):\n",
    "  for t, word in enumerate(d):\n",
    "    if word != 0:\n",
    "      decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a1cf67-af45-4624-8546-a7623b260d42",
   "metadata": {},
   "source": [
    "### Create word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeedac78-5272-400d-88e6-df607c85b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('../large_files/glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5d01e-26fb-4a6f-9fcd-60b8ff069dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('../large_files/glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98fd32-d39e-4a3e-be9a-763e18ba9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "  if i < MAX_VOCAB_SIZE:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca2b78-3f80-47d6-ac7a-7940cb30db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot the targets (can't use sparse cross-entropy)\n",
    "one_hot_targets = np.zeros((len(input_sequences), max_sequence_length, num_words))\n",
    "for i, target_sequence in enumerate(target_sequences):\n",
    "  for t, word in enumerate(target_sequence):\n",
    "    if word > 0:\n",
    "      one_hot_targets[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51743675-e552-4bd0-8258-042fef313dee",
   "metadata": {},
   "source": [
    "### Building the model to plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a15eb-2c2a-45e9-9f82-0c86a3800925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85796bde-53ef-49e7-bd10-46c43bd31038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map indexes back into real words\n",
    "# so we can view the results\n",
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943264f8-3546-4608-9da5-6f79fb589955",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b73b5-b18d-446b-9ff7-2eb800437ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=max_len_input,\n",
    "  # trainable=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6db8a-6b3b-4fd8-95d4-8083423f1e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332d1e79-d29b-47ab-a5ea-62e3abd1796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2322889-0530-448c-92eb-5c78be259e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding_layer(encoder_inputs_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc75545-58d0-4c51-b587-ca3402203c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Bidirectional(LSTM(\n",
    "  LATENT_DIM,\n",
    "  return_sequences=True,\n",
    "  # dropout=0.5 # dropout not available on gpu\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35956b75-5a24-40f1-b7b1-8ecf2426f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2289a5e9-a096-42f9-99d4-6d974988551c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decef467-f359-459f-802a-987f85268830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d5496-8f66-487b-9218-cc0b98dcad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder embedding with no pre trained embedding \n",
    "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67621c-d79f-4500-9995-c56482a18c9e",
   "metadata": {},
   "source": [
    "##### Setting up the Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6435bc7a-fcf4-4615-b6bb-f49a9c2e38d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention layers need to be global because\n",
    "# they will be repeated Ty times at the decoder\n",
    "# Following layer are setup to implement attention mechanism in seq2seq for transaltion\n",
    "attn_repeat_layer = RepeatVector(max_len_input)\n",
    "attn_concat_layer = Concatenate(axis=-1)\n",
    "attn_dense1 = Dense(10, activation='tanh')\n",
    "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
    "attn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f5cfc-c3e8-4c57-af3b-d1664d33c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(h, st_1):\n",
    "  # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n",
    "  # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n",
    " \n",
    "  # copy s(t-1) Tx times\n",
    "  # now shape = (Tx, LATENT_DIM_DECODER)\n",
    "  st_1 = attn_repeat_layer(st_1)\n",
    "\n",
    "  # Concatenate all h(t)'s with s(t-1)\n",
    "  # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n",
    "  x = attn_concat_layer([h, st_1])\n",
    "\n",
    "  # Neural net first layer\n",
    "  x = attn_dense1(x)\n",
    "\n",
    "  # Neural net second layer with special softmax over time\n",
    "  alphas = attn_dense2(x)\n",
    "\n",
    "  # \"Dot\" the alphas and the h's\n",
    "  # Remember a.dot(b) = sum over a[t] * b[t]\n",
    "  context = attn_dot([alphas, h])\n",
    "\n",
    "  return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed08ca-55ce-4dfc-8f48-b3832f1db37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the rest of the decoder (after attention)\n",
    "decoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3832b-3c89-444f-9d92-a812912f1882",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\n",
    "initial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\n",
    "context_last_word_concat_layer = Concatenate(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301739c3-ac7d-4020-ad58-4bd01a1011d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s, c will be re-assigned in each iteration of the loop\n",
    "s = initial_s\n",
    "c = initial_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356bf73-7f29-4ac2-9b45-0e94fda3a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect outputs in a list at first\n",
    "outputs = []\n",
    "for t in range(max_len_target): # Ty times\n",
    "  # get the context using attention\n",
    "  context = one_step_attention(encoder_outputs, s)\n",
    "\n",
    "  # we need a different layer for each time step\n",
    "  selector = Lambda(lambda x: x[:, t:t+1])\n",
    "  xt = selector(decoder_inputs_x)\n",
    "  \n",
    "  # combine \n",
    "  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "\n",
    "  # pass the combined [context, last word] into the LSTM\n",
    "  # along with [s, c]\n",
    "  # get the new [s, c] and output\n",
    "  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
    "\n",
    "  # final dense layer to get next word prediction\n",
    "  decoder_outputs = decoder_dense(o)\n",
    "  outputs.append(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85248fe-bedb-4696-a369-7f6aeef8b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'outputs' is now a list of length Ty\n",
    "# each element is of shape (batch size, output vocab size)\n",
    "# therefore if we simply stack all the outputs into 1 tensor\n",
    "# it would be of shape T x N x D\n",
    "# we would like it to be of shape N x T x D\n",
    "\n",
    "def stack_and_transpose(x):\n",
    "  # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
    "  x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
    "  x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a79e2-8870-4aa1-bf1f-bfb49ef2ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a stack and transpose layer\n",
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eaaccc-c6d4-45e4-bb58-f0c9c422ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model to plot loss and accuracy\n",
    "model = Model(\n",
    "  inputs=[\n",
    "    encoder_inputs_placeholder,\n",
    "    decoder_inputs_placeholder,\n",
    "    initial_s, \n",
    "    initial_c,\n",
    "  ],\n",
    "  outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ff26a-a671-4237-8306-84cdf589d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "  # both are of shape N x T x K\n",
    "  mask = K.cast(y_true > 0, dtype='float32')\n",
    "  out = mask * y_true * K.log(y_pred)\n",
    "  return -K.sum(out) / K.sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23343274-8b45-4816-a214-1983d6a74500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "  # both are of shape N x T x K\n",
    "  targ = K.argmax(y_true, axis=-1)\n",
    "  pred = K.argmax(y_pred, axis=-1)\n",
    "  correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
    "\n",
    "  # 0 is padding, don't include those\n",
    "  mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
    "  n_correct = K.sum(mask * correct)\n",
    "  n_total = K.sum(mask)\n",
    "  return n_correct / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb5999-0322-41fd-a0f0-0260a9edcd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[acc])\n",
    "# model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6050e42-4578-4997-b5c3-c08963160d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3310e-c4f6-403f-a9b9-93d6cfd36f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies\n",
    "plt.plot(r.history['accuracy'], label='acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578768b9-31bc-42ee-85c4-b6736e6cc028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to make the actual predictions\n",
    "# The encoder will be stand-alone\n",
    "# From this we will get our initial decoder hidden state\n",
    "# i.e. h(1), ..., h(Tx)\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c8ac0-3c71-45e0-b958-ffe74854509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we define a T=1 decoder model\n",
    "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac783a7e-68d5-464f-87d4-0639ef4a0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to loop over attention steps this time because there is only one step\n",
    "context = one_step_attention(encoder_outputs_as_input, initial_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbf246-c80c-4bea-8eeb-5257beedd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine context with last word\n",
    "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8506ae-4cc6-4a6a-92fd-f8f3a51761f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm and final dense\n",
    "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "decoder_outputs = decoder_dense(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02017ec1-5921-431e-af30-8ce6e4dc5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "decoder_model = Model(\n",
    "  inputs=[\n",
    "    decoder_inputs_single,\n",
    "    encoder_outputs_as_input,\n",
    "    initial_s, \n",
    "    initial_c\n",
    "  ],\n",
    "  outputs=[decoder_outputs, s, c]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d36a8e-4021-4fed-8555-43a3f90b7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_sequence(input_seq):\n",
    "  # Encode the input as state vectors.\n",
    "  enc_out = encoder_model.predict(input_seq)\n",
    "\n",
    "  # Generate empty target sequence of length 1.\n",
    "  target_seq = np.zeros((1, 1))\n",
    "  \n",
    "  # Populate the first character of target sequence with the start character.\n",
    "  # NOTE: tokenizer lower-cases all words\n",
    "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "\n",
    "  # if we get this we break\n",
    "  eos = word2idx_outputs['<eos>']\n",
    "\n",
    "\n",
    "  # [s, c] will be updated in each loop iteration\n",
    "  s = np.zeros((1, LATENT_DIM_DECODER))\n",
    "  c = np.zeros((1, LATENT_DIM_DECODER))\n",
    "\n",
    "\n",
    "  # Create the translation\n",
    "  output_sentence = []\n",
    "  for _ in range(max_len_target):\n",
    "    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
    "        \n",
    "\n",
    "    # Get next word\n",
    "    idx = np.argmax(o.flatten())\n",
    "\n",
    "    # End sentence of EOS\n",
    "    if eos == idx:\n",
    "      break\n",
    "\n",
    "    word = ''\n",
    "    if idx > 0:\n",
    "      word = idx2word_trans[idx]\n",
    "      output_sentence.append(word)\n",
    "\n",
    "    # Update the decoder input\n",
    "    # which is just the word just generated\n",
    "    target_seq[0, 0] = idx\n",
    "\n",
    "  return ' '.join(output_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51a827-c412-4e16-8547-a31886c843b0",
   "metadata": {},
   "source": [
    "### Executing the Model to perform translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329d249-4014-42c0-878c-ace3bc55fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "  # Do some test translations\n",
    "  i = np.random.choice(len(input_texts))\n",
    "  input_seq = encoder_inputs[i:i+1]\n",
    "  translation = decode_sequence(input_seq)\n",
    "  print('-')\n",
    "  print('Input sentence:', input_texts[i])\n",
    "  print('Predicted translation:', translation)\n",
    "  print('Actual translation:', target_texts[i])\n",
    "\n",
    "  ans = input(\"Continue? [Y/n]\")\n",
    "  if ans and ans.lower().startswith('n'):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81581b7e-c58c-46e2-a92e-df7d787ac2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
